{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e09ae760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd9016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "path = os.getcwd()\n",
    "classes =  [\"Circle\", \"Square\", \"Octagon\",\"Heptagon\", \"Nonagon\", \"Star\", \"Hexagon\", \"Pentagon\", \"Triangle\"]\n",
    "\n",
    "training_files = glob.glob(os.path.join(path,\"geometry_dataset\\training_images\\*.png\"))\n",
    "test_files = glob.glob(os.path.join(path,\"geometry_dataset\\test_images\\*.png\"))\n",
    "\n",
    "for training_file,test_file in zip(training_files,test_files):\n",
    "    os.remove(training_file)\n",
    "    os.remove(test_file)\n",
    "\n",
    "for class_ in classes:\n",
    "    image_location = os.path.join(path,\"geometry_dataset\\output\")\n",
    "    image_location = os.path.join(image_location,class_ + \"*.png\")\n",
    "    class_images = glob.glob(image_location)\n",
    "    print(class_,len(class_images))\n",
    "    random.shuffle(class_images)\n",
    "    for train_images in class_images[0:8000]:\n",
    "        shutil.copy(train_images,\"geometry_dataset/training_images/\"+str(class_))\n",
    "    for test_images in class_images[8000:]:\n",
    "        shutil.copy(test_images,\"geometry_dataset/test_images/\"+str(class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77bdb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1    = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2    = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1      = nn.Linear(614656, 128)\n",
    "        self.fc2      = nn.Linear(128, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04703358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    tot_loss = 0\n",
    "    correct  = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss   = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred    = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        tot_loss = tot_loss + loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), tot_loss/(batch_idx+1), 100.0*correct/((batch_idx+1)*batch_size)))\n",
    "\n",
    "    loss = tot_loss / len(train_loader)\n",
    "    acc = 100.0 * correct / (len(train_loader) * batch_size)\n",
    "    print('End of Epoch: {}'.format(epoch))\n",
    "    print('Training Loss: {:.6f}, Training Accuracy: {:.2f}%'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57dc8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_size, model, device, test_loader):\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output       = model(data)\n",
    "            tot_loss     += torch.nn.CrossEntropyLoss()(output, target).item()  # sum up batch loss\n",
    "            pred         = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct      += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss = tot_loss / len(test_loader)\n",
    "    acc = 100.0 * correct / (len(test_loader) * batch_size)\n",
    "    print('Test Loss: {:.6f}, Test Accuracy: {:.2f}%'.format(loss, acc))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79a3451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/72000 (0%)]\tLoss: 2.202981, Accuracy: 13.00%\n",
      "Train Epoch: 1 [5000/72000 (7%)]\tLoss: 5.715325, Accuracy: 10.63%\n",
      "Train Epoch: 1 [10000/72000 (14%)]\tLoss: 3.974327, Accuracy: 10.69%\n",
      "Train Epoch: 1 [15000/72000 (21%)]\tLoss: 3.386461, Accuracy: 11.03%\n",
      "Train Epoch: 1 [20000/72000 (28%)]\tLoss: 3.090663, Accuracy: 11.21%\n",
      "Train Epoch: 1 [25000/72000 (35%)]\tLoss: 2.912855, Accuracy: 11.15%\n",
      "Train Epoch: 1 [30000/72000 (42%)]\tLoss: 2.794087, Accuracy: 11.07%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test(batch_size,model, device, test_loader)\n\u001b[0;32m     19\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(batch_size, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m      9\u001b[0m loss   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(output, target)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m pred    \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 1\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(),\"geometry_dataset/training_images\"),transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(os.getcwd(),\"geometry_dataset/test_images\"),transform=transform)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(batch_size, model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_acc = test(batch_size,model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    if args.save_model: # save new model after every epoch\n",
    "        torch.save(model.state_dict(), \"0602-656377418-Garg.pt\")\n",
    "\n",
    "    if test_loss < 1e-4 or math.isclose(test_acc, 100.0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cde57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
